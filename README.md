# ğŸ–¼ï¸ Image_Caption_Generator
More like image description than caption!

A modular, Streamlit-powered app that generates natural language captions for uploaded images using a pretrained BLIP model. Built with PyTorch and designed for clarity, scalability, and sharing.

Upload an image and get a descriptive caption in seconds.

## ğŸ“ Project Structure

```
Image_Caption/
â”œâ”€â”€ main.py                  # Entry point for Streamlit app
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ interface.py         # Streamlit UI logic
â”‚   â”œâ”€â”€ model.py             # BLIP model loading and caption generation
â”‚   â”œâ”€â”€ utils.py             # Image preprocessing helpers
â”‚   â”œâ”€â”€ __pycache__/         # Python cache (ignored)
â”‚   â””â”€â”€ .ipynb_checkpoints/  # Jupyter backups (ignored)
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ *.png                # Sample/demo images
â”œâ”€â”€ __pycache__/             # Python cache (ignored)
â””â”€â”€ .ipynb_checkpoints/      # Jupyter backups (ignored)
```


## ğŸ› ï¸ Technologies Used 

* Python 3.x
* Streamlit
* Libraries:
  
  * `PyTorch`
  * `transformers`
  * `Pillow`
  * `requests`
  * `os`
  * `io`
    
* Model:

   BLIP (Salesforce/blip-image-captioning-base) via Hugging Face

## ğŸ§  Model

Uses the [BLIP (Bootstrapped Language Image Pretraining)](https://github.com/salesforce/BLIP) model for zero-shot image captioning. The model is loaded via Hugging Face Transformers and optimized for inference.

## ğŸ› ï¸ Setup

1. Clone the repo  
2. Create a virtual environment  
3. Install dependencies
4. Run the app:

```bash
streamlit run main.py
```

## ğŸ“¦ Dependencies

- `PyTorch`
- `transformers`
- `streamlit`
- `Pillow`
- `requests`


## ğŸ§¹ Future Improvements

- Add support for multiple captions per image  
- Integrate image enhancement or resizing  
- Deploy via Hugging Face Spaces or Streamlit Cloud  
- Add drag-and-drop upload and caption history

## ğŸ“„ License

MIT License. See `LICENSE` for details.

---




